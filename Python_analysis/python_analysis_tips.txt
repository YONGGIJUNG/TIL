https://subinium.github.io/MLwithPython-3-5/

https://rfriend.tistory.com/414

https://nittaku.tistory.com/113



소스) 파이썬 - 가우스 혼합 모델(군집 할당 plot_gmm)

 

프로파일
 꿈주리  ・  2020. 4. 24. 22:29 
URL 복사  이웃추가 

본문 기타 기능



   


  





%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np
  



plot_gmm function

 




from matplotlib.patches import Ellipse

def draw_ellipse(position, covariance, ax=None, **kwargs):
    """Draw an ellipse with a given position and covariance"""
    ax = ax or plt.gca()
    
    # Convert covariance to principal axes
    if covariance.shape == (2, 2):
        U, s, Vt = np.linalg.svd(covariance)
        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))
        width, height = 2 * np.sqrt(s)
    else:
        angle = 0
        width, height = 2 * np.sqrt(covariance)
    
    # Draw the Ellipse
    for nsig in range(1, 4):
        ax.add_patch(Ellipse(position, nsig * width, nsig * height,
                             angle, **kwargs))
                             
def plot_gmm(gmm, X, label=True, ax=None):
    ax = ax or plt.gca()
    labels = gmm.fit(X).predict(X)
    if label:
        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)
    else:
        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)
    ax.axis('equal')
    
    w_factor = 0.2 / gmm.weights_.max()
    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):
        draw_ellipse(pos, covar, alpha=w * w_factor)
  




from sklearn.datasets import make_moons
Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)
plt.scatter(Xmoon[:, 0], Xmoon[:, 1]);
  



  





from sklearn.mixture import GaussianMixture
gmm2 =  GaussianMixture(n_components=2, covariance_type='full', random_state=0)
plot_gmm(gmm2, Xmoon)
  



  





gmm16 = GaussianMixture(n_components=16, covariance_type='full', random_state=0)
plot_gmm(gmm16, Xmoon, label=False)
  



  




​[출처] 소스) 파이썬 - 가우스 혼합 모델(군집 할당 plot_gmm)|작성자 꿈주리







https://teddylee777.github.io/pandas/10minutes-to-pandas-%EB%B6%80%EB%8F%99%EC%82%B0%EC%8B%A4%EC%A0%9C%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%99%9C%EC%9A%A9%ED%95%98%EA%B8%B0

https://jsideas.net/daily_to_weekly/

https://blog.naver.com/PostView.nhn?blogId=okkam76&logNo=221372134987&parentCategoryNo=&categoryNo=27&viewDate=&isShowPopularPosts=false&from=postList

https://stackoverrun.com/ko/q/9675748

https://wikidocs.net/46755

https://3months.tistory.com/296

https://sikaleo.tistory.com/40

https://data-make.tistory.com/139


cols = df.columns
cols_fnl = cols.drop(['area','city'])
df2.columns=['area','city','pdate','cnt']



data.corr()
%matplotlib inline   #쥬피터노트북에서 이미지 표시가능하게 하는 쥬피터노트북 매직함수
import matplotlib.pyplot as plt 
import seaborn as sns  
plt.figure(figsize=(15,15))
sns.heatmap(data = data.corr(), annot=True, 
fmt = '.2f', linewidths=.5, cmap='Blues')

df = raw.corr()

sns.clustermap(df, 
               annot = True,      # 실제 값 화면에 나타내기
               cmap = 'RdYlBu_r',  # Red, Yellow, Blue 색상으로 표시
               vmin = -1, vmax = 1, #컬러차트 -1 ~ 1 범위로 표시
              )

			  
# 삼각형
df = raw.corr()
# 그림 사이즈 지정
fig, ax = plt.subplots( figsize=(7,7) )

# 삼각형 마스크를 만든다(위 쪽 삼각형에 True, 아래 삼각형에 False)
mask = np.zeros_like(df, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# 히트맵을 그린다
sns.heatmap(df, 
            cmap = 'RdYlBu_r', 
            annot = True,   # 실제 값을 표시한다
            mask=mask,      # 표시하지 않을 마스크 부분을 지정한다
            linewidths=.5,  # 경계면 실선으로 구분하기
            cbar_kws={"shrink": .5},# 컬러바 크기 절반으로 줄이기
            vmin = -1,vmax = 1   # 컬러바 범위 -1 ~ 1
           )  
plt.show()


data = df.pivot_table(index = 'size', columns = 'day', values = 'tip', aggfunc = 'sum' )
data.head()
sns.heatmap(data)   # 기본 생성


# 값 표시할때

sns.heatmap(data, 
           annot = True, fmt = '.0f')

# 글자 폰드 조정하기(글자 크게 보이기)
sns.heatmap(data, 
            annot = True, fmt = '.0f', 
            annot_kws = {'size' : 20})

# 글자 폰트 작게
sns.heatmap(data, 
            annot = True, fmt = '.0f', 
            annot_kws = {'size' : 7})


fmt1 = '%d x %d = %d'%(a,b,a*b)
print(fmt1)   #  5 x 8 = 40
fmt2 = '{} x {} = {}'.format(a,b,a*b)

			  
fmt4 = f'{a} x {b} = {a*b}'


import seaborn as sns

# 데이터 준비하기 (하고 싶은 데이터프레임으로 변경 가능)
tips = sns.load_dataset("tips")

# plot swarmplot
sns.swarmplot(data=tips, x="day", y="total_bill")
# plot boxplot
sns.boxplot(data=tips, x="day", y="total_bill",  
            showcaps=False,             # 박스 상단 가로라인 보이지 않기
            whiskerprops={'linewidth':0}, # 박스 상단 세로 라인 보이지 않기 
            showfliers=False,           # 박스 범위 벗어난 아웃라이어 표시하지 않기
            boxprops={'facecolor':'None'}, # 박스 색상 지우기
        )			  


https://m.blog.naver.com/PostView.nhn?blogId=kiddwannabe&logNo=221205309816&proxyReferer=https:%2F%2Fwww.google.com%2F


# 마커 추가하기
folium.Marker([37.5536067,126.9674308],    # 서울역위치
              tooltip="서울역(마우스올리면보여짐)",
              popup="서울역(클릭하면 보여짐)",
              ).add_to(m)



from folium.plugins import MiniMap

# 지도 생성하기
m = folium.Map(location=[37.5536067,126.9674308],   # 기준좌표: 서울역
               zoom_start=12)

# 미니맵 추가하기
minimap = MiniMap() 
m.add_child(minimap)

# 마커 추가하기
folium.Marker([37.5536067,126.9674308],    # 서울역위치
              tooltip="서울역(마우스올리면보여짐)",
              popup="서울역(클릭하면 보여짐)",
              ).add_to(m)
m		



from tqdm.notebook import tqdm

for i in tqdm(range(10)):
    print(i)
    time.sleep(0.5)


! pip  install matplotlib=3.0.3
# 버전 지정해서 설치하기
! pip install matplotlib=3.1.2

# 최신 버전으로 업그레이드하기
! pip install matplotlib --upgrade


# 그림 사이즈 지정
fig, ax = plt.subplots( figsize=(10,60) )
# 히트맵 그리기
sns.heatmap(data = data)

# 축 조정
b, t = plt.ylim() # discover the values for bottom and top
b += 0.5 # Add 0.5 to the bottom
t -= 0.5 # Subtract 0.5 from the top
plt.ylim(b, t) # update the ylim(bottom, top) values

plt.show() 



! pip install python-pptx

# 필요한 라이브러리 불러오기
from pptx import Presentation
from pptx.enum.text import PP_ALIGN   # 정렬 설정하기
from pptx.util import Pt      # Pt 폰트사이즈

# pptx 파일 열기
pptx_fpath = './명패포맷.pptx'
prs = Presentation(pptx_fpath)

# 슬라이드 지정하기
slide_num = 0
slide = prs.slides[slide_num]

# 슬라이드 내 shape 사전 만들기
shapes_list = slide.shapes
shape_index = {}
for i, shape in enumerate(shapes_list):
    shape_index[ shape.name ] = i
print(shape_index)   # {'Box_down': 0, 'Box_up': 1, 'name2': 2, 'name1': 3}

def text_on_shape(shape, input_text, font_size = 95, font_color = 'black', bold = True):

    # shape 내 텍스트 프레임 선택하기 & 기존 값 삭제하기
    text_frame = shape.text_frame
    text_frame.clear()

    # 문단 선택하기 
    p = text_frame.paragraphs[0]

    # 정렬 설정: 중간정렬
    p.alighnment = PP_ALIGN.CENTER  

    # 텍스트 입력 / 폰트 설정
    run = p.add_run()   
    run.text = input_text
    font = run.font
    font.size = Pt(font_size)
    font.bold = bold 
    font.name = None

# 하나의 shape 선택하기
shape_name = 'name1'
shape_select = shapes_list[ shape_index[ shape_name ]]    

text_on_shape(shape_select, '홍길동')


save_file = './홍길동.pptx'
prs.save(save_file)





만약 abcdef.py 파일 내에,   함수 f1, f2 가 있다면

아래와 같은 import 문으로 이 함수들을 다시 사용할 수 있어요. 

​

from abcdef import f1  # f1 함수만 사용할때
from abcdef import f1, f2    # f1 함수, f2 함수를 사용할때
from abcdef import *   # abcdef.py 파일내 있는 모든 함수/변수를 불러올때




# 라이브러리 설치
! pip install pytube3

from pytube import YouTube
video_url = 'https://www.youtube.com/watch?v=Zg3j6anDU6U'
yt = YouTube(video_url)

print("[영상 제목]", yt.title)  # 영상제목
print("[영상 게시자]", yt.author) # 영상 게시자
print("[조회수]", yt.views)
print("[평균평점]", yt.rating) # 평균 평점
print("[영상길이(초)]", yt.length)
print("[연령제한여부]", yt.age_restricted)
print("[영상 설명]", yt.description) # 영상 설명
print("[썸네일URL]", yt.thumbnail_url) # 썸네일 url 주소


yt.streams.all()

# 전송 포맷 중 첫번째 선택
stream = yt.streams.all()[0]
stream   # <Stream: itag="18" mime_type="video/mp4" res="360p" fps="30fps" vcodec="avc1.42001E" acodec="mp4a.40.2" progressive="True" type="video">

# 내가 원하는 영상만 선택하기
# 음성만 선택시
yt.streams.filter(only_audio = True).all()
# 내가 원하는 영상만 선택하기
# mp4만
yt.streams.filter(file_extension = 'mp4').all( )


stream = yt.streams.filter(file_extension = 'mp4').all( )[0]
stream   # <Stream: itag="18" mime_type="video/mp4" res="360p" fps="30fps" vcodec="avc1.42001E" acodec="mp4a.40.2" progressive="True" type="video">


# 영상 다운로드
stream.download()

# 파일 이름 지정 
## 폴더지정 가능
stream.download(output_path='test', filename = 'KOBE', filename_prefix= 'R.I.P_')

yt.captions.all()
caption = yt.captions.get_by_language_code('ko')

# 언어로 자막 선택하기
## 한글 자막 1순위로 선택하기. 만약 한글 자막이 없다면 자막 리스트 중 첫 번째 자막 선택하기
caption = yt.captions.get_by_language_code('ko')
if caption == None:
    caption = yt.captions.all()[0]

# 자막 살펴보기(xml 포맷)
caption.xml_captions

# 자막 살펴보기(srt 포맷)
print(caption.generate_srt_captions())

# 자막 다운받기: download("파일명")
caption.download( yt.title )





sns.scatterplot(x = 'total_bill', y = 'tip', data = raw, hue = 'sex')
sns.lineplot(x = 'total_bill', y = 'tip', data = raw, hue = 'sex') 

sns.lmplot(x = 'total_bill', y = 'tip', data = raw, hue = 'sex', col='day')
sns.lmplot(x = 'total_bill', y = 'tip', data = raw, hue = 'sex', col='day', col_wrap= 2)



import pandas as pd
import pandas_profiling

# 레포트 생성 --> html 파일로 저장하기  
## 한글이 깨지시는 분은 제일 아래 코드로 실행해주세요
raw = pd.read_excel('파일명.xlsx')
report = raw.profile_report()
report.to_file('report.html')



import pandas as pd
import pandas_profiling

############ 한글 폰트 지정하는 부분 #################
import matplotlib
from matplotlib import font_manager, rc
import platform

if platform.system() == 'Windows':
# 윈도우인 경우
    font_name = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
    rc('font', family=font_name)
else:    
# Mac 인 경우
    rc('font', family='AppleGothic')
    
matplotlib.rcParams['axes.unicode_minus'] = False   
#그래프에서 마이너스 기호가 표시되도록 하는 설정입니다.


# 레포트 생성 --> html 파일로 저장하기  
## 한글이 깨지시는 분은 제일 아래 코드로 실행해주세요
raw = pd.read_excel('파일명.xlsx')
report = raw.profile_report()
report.to_file('report.html')	
			  
			  
pip install pandas_profiling
			  

pandas profiling

pandas-profiling.github.io

import pandas-profiling as pp

df.profile_report()



# 토픽 정리하기
TOPIC_change = {    '조작성' : '사용성',    '수납/공간':'사용성',    '용량' : '사용성',    '냄새' : '사용성',
    '관리' : '사용성',    '활용도' : '사용성',    '휴대성' : '사용성',    '관리' : '사용성',    '색상' : '디자인',
    '기능' : '성능',    '세척' : '관리',       '제품구성' : '가격'}
raw['topic_modi'] = raw['topic'].apply(lambda x: TOPIC_change[x] if x in TOPIC_change.keys() else x)

topic_pivot = pd.pivot_table(data = raw, index = 'brand', columns = 'topic_modi',values = 'product_id', aggfunc='count',fill_value=0)
topic_sorting = raw.pivot_table(index = 'topic_modi', values = 'score', aggfunc='count').sort_values(by = 'score', ascending = False).index
topic_pivot[topic_sorting]


score_df = pd.pivot_table(data = raw, index = 'brand', columns = 'topic',values = 'score', aggfunc='mean',fill_value=0, )
score_df = score_df[topic_sorting]
score_df.style.format("{:.3}")

import plotly.graph_objects as go

categories = score_df.columns[1:]
fig = go.Figure()

for i in range(len(score_df)):
    fig.add_trace(go.Scatterpolar(
          r=score_df.iloc[i,1:],
          theta=categories,
          fill='toself',
          name=score_df.index[i],
        opacity = 0.7     
    ))
    
fig.update_layout(
  polar=dict(
    radialaxis=dict(
      visible=True,
      range=[4.4, 5]
    )),
  showlegend=True
)



pip install jupyter_contrib_nbextensions      # 라이브러리 설치
jupyter contrib nbextension install --user    # 쥬피터노트북에서 보일 수 있도록 등록



http://localhost:8888/nbextensions  


# 데이터 값 실수. 소수점 두째자리까지 표시
pd.options.display.float_format = '{:.2f}'.format





# real = 실제 값,   prediction = 예측한 값
from sklearn.metrics import confusion_matrix
confusion_matrix(real, prediction)  #confusion matrix 표시


from sklearn.metrics import precision_score, recall_score, f1_score
precision_score(real, prediction)  #precision(정밀도)
recall_score(real, prediction)     #sensitivity(민감도), recall(재현율)
f1_score(real, prediction)         # F1 score(정밀도, 민감도 조화평균)


① 뺨을 때렸는데(P) 모기가 있었다면  TP  (Positive 가 True 다)  ▶ 아싸~! 쾌감!
② 뺨을 때렸는데(P) 모기가 없었다면 ㅠ  FP(Positive 가 False 다) ▶ 아프기만 하고.. ㅠ
③ 뺨을 안때렸는데(N) 모기가 있었따면? ㅠ FN(Negative 가 False) ▶ 간지럽고.. 짜증나고. ㅠ
④ 뺨을 안때렸는데(N) 모기가 없었다면   TN(Negative 가 True)  ▶ 숙면 ㅎ

Precision_score(정밀도)는, 
내가 모기가 있다고 예상하고 손을 휘둘렀을때,  실제로 모기가 있어서 잡은 비율을 말합니다.

Precision_score =     TP / P      =        TP / (FP + TP)

이 비율이 높을 수록,  짜증은 감소하고 환희는 증가하겠죠
원샷~원킬~!!


2. Recall_score(재현율)

두 번째는 Recall_score 혹은 Sensitivity 라고 부르는 방식입니다. 
실제로 모기가 있을때(True), 잘 잡느냐(P) 를 평가하는 방법이죠. 


recall_score = TP  / ( True)  = TP / (FN + TP)

이 지표가 높을 수록 모기에 물릴 확률이 적어지겠죠. 


이렇게 예측모델의 정확도를 측정하는 방법은 하나가 아닙니다. 
모기에 물리지 않게 하는것이 중요한지(recall), 
원샷원킬로  휘두를때 잡을 확률이 높은 것이 중요한지(precision) 는 답이 없습니다. 

분류 예측하는 상품/서비스의 특성에 따라 많이 달라지겠죠. 
※ 정밀도와 재현율 두 가지 지표를 비교하기 쉽게 하나의 지표로 전환하여 사용하기도 합니다. 
 ex) 정밀도와 재현율의 조화평균으로 계산한 F1 이 있습니다

 
 
word_coocs = [
    ('부동산','가격',20),
    ('부동산','아파트',10),
    ('아파트','가격', 9),
    ('아파트','강남', 7),
    ('아파트','분양',5),
    ('부동산','토지',5),
    ('아파트','전세', 3),
    ('부동산','대출', 2),
    ('신혼부부','대출',2),
    ('강남','전세',1)
]

# 그래프 그리기 준비
import networkx as nx
import matplotlib.pyplot as plt
import sys


def sna_graph(word_coocs, NETWORK_MAX):
    
    G = nx.Graph()
    i = 0
    
    # edge 를 생성한다
    for word1, word2, count in word_coocs:
        i +=1
        if i > NETWORK_MAX: break
        
        G.add_edge(word1, word2, weight=count)
    
    # MST 모델을 만든다. 
    T = nx.minimum_spanning_tree(G)
    nodes = nx.nodes(T)
    degrees = nx.degree(T)
    
    # node 사이즈를 정해준다
    node_size = []
    for node in nodes:
        ns = degrees[node]*100
        node_size.append(ns)

    # 폰트 정리
    if sys.platform in ["win32", "win64"]:
        font_name = "malgun gothic"
    elif sys.platform == "darwin":
        font_name = "AppleGothic"
        
    # 분석결과를 화면에 표시한다
    plt.figure(figsize=(16,12))    
    nx.draw_networkx(T,
           pos=nx.fruchterman_reingold_layout(G, k=0.5),
           node_size=node_size,
           node_color="yellow",
           font_family=font_name,
           label_pos=1, #0=head, 0.5=center, 1=tail
            with_labels=True,
            font_size=12 )

    plt.axis("off")
    plt.show()



import tarfile

fname = './압축파일이름.확장자'  # 압축 파일을 지정해주고   
ap = tarfile.open(fname)      # 열어줍니다. 

ap.extractall(압축풀경로)         # 그리고는 압축을 풀어줍니다. 
# () 안에는 풀고 싶은 경로를 넣어주면 되요. 비워둘 경우 현재 경로에 압축 풉니다. 
 
ap.close()                  # 압축파일은 이제 닫아줍니다. 




import pandas as pd

data = pd.DataFrame(source)

#중복된 행인지 점검하기(동일한 데이터가 있는지)
display(data.duplicated())

#중복된 행의 데이터만 표시하기
display(data[data.duplicated()])

#중복된 행은 하나만 남기고 제거하기
data.drop_duplicates(inplace = True)





import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders


# 지메일 아이디,비번 입력하기
email_user = '<ID>@gmail.com'      #<ID> 본인 계정 아이디 입력
email_password = '<PASSWORD>'      #<PASSWORD> 본인 계정 암호 입력
email_send = '<받는곳주소>'         # <받는곳주소> 수신자 이메일 abc@abc.com 형태로 입력

# 제목 입력
subject = '안녕하세요. 데이터공방입니다 ' 

msg = MIMEMultipart()
msg['From'] = email_user
msg['To'] = email_send
msg['Subject'] = subject

# 본문 내용 입력
body = '데이터공방입니다. bog.naver.com/kiddwannabe'
msg.attach(MIMEText(body,'plain'))

############### ↓ 첨부파일이 없다면 삭제 가능  ↓ ########################
# 첨부파일 경로/이름 지정하기
filename='첨부파일.csv'  
attachment  =open(filename,'rb')

part = MIMEBase('application','octet-stream')
part.set_payload((attachment).read())
encoders.encode_base64(part)
part.add_header('Content-Disposition',"attachment; filename= "+filename)
msg.attach(part)
############### ↑ 첨부파일이 없다면 삭제 가능  ↑ ########################

text = msg.as_string()
server = smtplib.SMTP('smtp.gmail.com',587)
server.starttls()
server.login(email_user,email_password)

server.sendmail(email_user,email_send,text)
server.quit()





# \xa0, \xa9 를 없애줌
data = data.applymap(lambda x: x.replace('\xa0','').replace('\xa9',''))  

# encoding: cp949 로 전환--> 엑셀에서 깨지지 않게
data.to_csv(fname, encoding='cp949')     



(버거킹+맥도날드+KFC)÷롯데리아 = 한 도시 발전 수준.

한 도시의 발전 수준은 (버거킹의 갯수+맥도날드의 갯수+KFC의 갯수)/롯데리아의 갯수를 계산하여 높게 나올수록 더 발전된 도시라고 할 수 있다

https://nbviewer.jupyter.org/gist/hyeshik/cf9f3d7686e07eedbfda?revision=6
https://twitter.com/godtsune_miku/status/513648274406789120


import urllib.request
import json
import pandas as pd
import bs4

response = urllib.request.urlopen('http://www.burgerking.co.kr/api/store/searchmap/empty/?areacd=')
bgk_data = json.loads(response.read().decode('utf-8'))
bgk_tbl = pd.DataFrame(bgk_data)
bgk_tbl.head()

bgk_locs = pd.DataFrame(bgk_tbl['NewAddr'].apply(lambda v: v.split()[:2]).tolist(),
                        columns=('d1', 'd2'))
bgk_locs.head()

bgk_locs['d1'].unique()

d1_aliases = """서울시:서울특별시 충남:충청남도 강원:강원도 경기:경기도 충북:충청북도 경남:경상남도 경북:경상북도
전남:전라남도 전북:전라북도 제주도:제주특별자치도 제주:제주특별자치도 대전시:대전광역시 대구시:대구광역시 인천시:인천광역시
광주시:광주광역시 울산시:울산광역시"""
d1_aliases = dict(aliasset.split(':') for aliasset in d1_aliases.split())
bgk_locs['d1'] = bgk_locs['d1'].apply(lambda v: d1_aliases.get(v, v))


bgk_locs['d1'].unique()

bgk_locs[bgk_locs['d1'] == '수원시']

bgk_locs.iloc[101] = ['경기도', '수원시']

bgk_locs['d2'].unique()

B = bgk_locs.apply(lambda r: r['d1'] + ' ' + r['d2'], axis=1).value_counts()
B.head()

MCDONALDS_URL = 'http://www.mcdonalds.co.kr/www/kor/findus/district.do?sSearch_yn=Y&skey=2&pageIndex={page}&skeyword={location}'

def search_mcdonalds_stores_one_page(location, page):
    response = urllib.request.urlopen(
        MCDONALDS_URL.format(location=urllib.parse.quote(location.encode('utf-8')), page=page))
    mcd_data = response.read().decode('utf-8')
    soup = bs4.BeautifulSoup(mcd_data)
    
    ret = []
    for storetag in soup.findAll('dl', attrs={'class': 'clearFix'}):
        storename = storetag.findAll('a')[0].contents[-1].strip()
        storeaddr = storetag.findAll('dd', attrs={'class': 'road'})[0].contents[0].split(']')[1]
        storeaddr_district = storeaddr.split()[:2]
        ret.append([storename] + storeaddr_district)

    return pd.DataFrame(ret, columns=('store', 'd1', 'd2')) if ret else None

# 여러 페이지를 쭉 찾아서 안 나올 때 까지 합친다.
def search_mcdonalds_stores(location):
    from itertools import count
    
    found = []
    for pg in count():
        foundinpage = search_mcdonalds_stores_one_page(location, pg+1)
        if foundinpage is None:
            break
        found.append(foundinpage)

    return pd.concat(found)
	
search_mcdonalds_stores('전라북도').head()


found = []
for distr in bgk_locs['d1'].unique():
    found.append(search_mcdonalds_stores(distr))
mcd_tbl = pd.concat(found)
mcd_tbl['store'].value_counts().head()

mcd_tbl = mcd_tbl.drop_duplicates(subset=['store'])
M = mcd_tbl.apply(lambda r: r['d1'] + ' ' + r['d2'], axis=1).value_counts()
M.head()

kfc_dists = "강원 경기 경남 경북 광주 대구 대전 부산 서울 울산 인천 전남 전북 제주 충남 충북".split()
KFC_DISTSEARCH_URL = 'http://www.kfckorea.com/store/store_addr_search.asp?addr_div=gugun&sido={location}'

def kfc_search_subdists(location):
    response = urllib.request.urlopen(
        KFC_DISTSEARCH_URL.format(location=urllib.parse.quote(location.encode('utf-8'))))
    kfc_data = response.read().decode('utf-8')
    soup = bs4.BeautifulSoup(kfc_data)
    return list(filter(None, [tag.attrs['value'] for tag in soup.findAll('option')]))
kfc_alldist = [(d, subd) for d in kfc_dists for subd in kfc_search_subdists(d)]
kfc_alldist[:5], len(kfc_alldist)

KFC_STORESEARCH_URL = ('http://www.kfckorea.com/store/store_search.asp?sales_24_yn_=&'
                       'sales_wifi_yn_=&sales_order_group_yn_=&sales_park_yn_=&sales_subway_yn_=&'
                       'sales_mart_in_yn_=&searchFlag=0&addr_div1={div1}&addr_div2={div2}&keyword=')

def kfc_search_stores_in_dist(d1, d2):
    response = urllib.request.urlopen(
        KFC_STORESEARCH_URL.format(div1=urllib.parse.quote(d1.encode('utf-8')),
                                   div2=urllib.parse.quote(d2.encode('utf-8'))))
    return json.loads(response.read().decode('utf-8'))['store']
found = []
for d1, d2 in kfc_alldist:
    found.extend(kfc_search_stores_in_dist(d1, d2))
kfc_tbl = pd.DataFrame(found)
kfc_tbl.head()


kfc_locs = pd.DataFrame(kfc_tbl['old_addr1'].apply(
    lambda v: v.replace('&nbsp;', ' ').replace('&#160;', ' ').replace('광주 광역', '광주광역').split()[:2]).tolist(),
              columns=('d1', 'd2'))
kfc_locs['d1'].unique()

d1_aliases = """서울시:서울특별시 충남:충청남도 강원:강원도 경기:경기도 충북:충청북도 경남:경상남도 경북:경상북도
전남:전라남도 전북:전라북도 제주도:제주특별자치도 제주:제주특별자치도 대전시:대전광역시 대구시:대구광역시 인천시:인천광역시
광주시:광주광역시 울산시:울산광역시 광주:광주광역시 대구:대구광역시 대전:대전광역시 부산:부산광역시 부산시:부산광역시
인천:인천광역시 서울:서울특별시 울산:울산광역시"""
d1_aliases = dict(aliasset.split(':') for aliasset in d1_aliases.split())
kfc_locs['d1'] = kfc_locs['d1'].apply(lambda v: d1_aliases.get(v, v))
kfc_locs['d1'].unique()

kfc_locs['d2'].unique()

K = kfc_locs.apply(lambda r: r['d1'] + ' ' + r['d2'], axis=1).value_counts()
K.head()

BMK = pd.DataFrame({'B': B, 'M': M, 'K': K}).fillna(0)
BMK['total'] = BMK.sum(axis=1)
BMK = BMK.sort('total', ascending=False)
BMK.head(10)


from matplotlib import pyplot as plt
from matplotlib import rcParams, style
style.use('ggplot')
rcParams['font.size'] = 12

plt.figure(figsize=(4, 3))
BMK.sum(axis=0).iloc[:3].plot(kind='bar')

import scipy.stats
fig = plt.figure(figsize=(9, 3))

def plot_nstores(b1, b2, label1, label2):
    plt.scatter(BMK[b1] + np.random.random(len(BMK)),
                BMK[b2] + np.random.random(len(BMK)),
                edgecolor='none', alpha=0.75, s=6, c='black')
    plt.xlim(-1, 15)
    plt.ylim(-1, 15)
    plt.xlabel(label1)
    plt.ylabel(label2)
    
    r = scipy.stats.pearsonr(BMK[b1], BMK[b2])
    plt.annotate('r={:.3f}'.format(r[0]), (10, 12.5))

ax = fig.add_subplot(1, 3, 1)
plot_nstores('B', 'M', 'Burger King', "McDonald's")

ax = fig.add_subplot(1, 3, 2)
plot_nstores('B', 'K', 'Burger King', 'KFC')

ax = fig.add_subplot(1, 3, 3)
plot_nstores('M', 'K', "McDonald's", 'KFC')

plt.tight_layout()


plt.figure(figsize=(4, 3))
for col, label in [('B', 'Burger King'), ('K', 'KFC'), ('M', "McDonald's")]:
    cumulv = np.cumsum(sorted(BMK[col], reverse=True)) / BMK[col].sum()
    plt.plot(cumulv, label='{} ({})'.format(label, int(BMK[col].sum())))
plt.legend(loc='best')
plt.xlabel('Number of districts (si/gun/gu)')
plt.ylabel('Cumulative fraction')



LOTTERIA_URL = 'http://www.lotteria.com/Shop/Shop_Ajax.asp'
LOTTERIA_VALUES = {
    'Page': 1, 'PageSize': 2000, 'BlockSize': 2000,
    'SearchArea1': '', 'SearchArea2': '', 'SearchType': "TEXT",
    'SearchText': '', 'SearchIs24H': '', 'SearchIsWifi': '',
    'SearchIsDT': '', 'SearchIsHomeService': '', 'SearchIsGroupOrder': '',
    'SearchIsEvent': ''}
LOTTERIA_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:12.0) Gecko/20100101',
    'Host': 'www.lotteria.com',
    'Accept': 'text/html, */*; q=0.01',
    'Accept-Language': 'en-us,en;q=0.5',
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    'X-Requested-With': 'XMLHttpRequest',
    'Referer': 'http://www.lotteria.com/Shop/Shop_List.asp?Page=1&PageSize=2000&BlockSize=2000&Se'
               'archArea1=&SearchArea2=&SearchType=TEXT&SearchText=&SearchIs24H=&SearchIsWifi=&Se'
               'archIsDT=&SearchIsHomeService=&SearchIsGroupOrder=&SearchIsEvent=',
}
postdata = urllib.parse.urlencode(LOTTERIA_VALUES).encode('utf-8')
req = urllib.request.Request(LOTTERIA_URL, postdata, LOTTERIA_HEADERS)
response = urllib.request.urlopen(req)
ltr_data = response.read().decode('utf-8')
soup = bs4.BeautifulSoup(ltr_data)
found = []
for tag in soup.findAll('tr', {'class': 'shopSearch'}):
    subtag = [tag.findAll('td', {'style': 'padding-right:10px;'}
                          )[i].contents[0].contents[0]
              for i in (0, 1)]
    found.append([subtag[0]] + subtag[1].replace('광주 광역', '광주광역').split()[:2])
ltr_tbl = pd.DataFrame(found, columns=('storename', 'd1', 'd2'))
ltr_tbl.head()


ltr_tbl['d1'].unique()

d1_aliases = """강원:강원도 충북:충청북도 부산:부산광역시 경기:경기도 전남:전라남도 경북:경상북도
대구:대구광역시 서울:서울특별시 세종:세종특별자치시 경남:경상남도 충남:충청남도 대전:대전광역시
울산:울산광역시 제주:제주특별자치도 인천:인천광역시 전북:전라북도 광주:광주광역시 대전시:대전광역시
충남시:충청남도 청북:충청북도"""
d1_aliases = dict(aliasset.split(':') for aliasset in d1_aliases.split())
ltr_tbl['d1'] = ltr_tbl['d1'].apply(lambda v: d1_aliases.get(v, v))
ltr_tbl['d1'].unique()

ltr_tbl['d2'].unique()


d2_aliases = """나주시금천면:나주시 장성군장성읍:장성군 고흥군고흥읍:고흥군 기장군정관면:기장군
창녕군남지읍:창녕군 임실군임실읍:임실군 원주시지정면:원주시 진구:부산진구 논산시연무읍:논산시"""
d2_aliases = dict(aliasset.split(':') for aliasset in d2_aliases.split())
ltr_tbl['d2'] = ltr_tbl['d2'].apply(lambda v: d2_aliases.get(v, v))
ltr_tbl['d2'].unique()

ltr_tbl[ltr_tbl['d2'].apply(lambda v: v[-1] not in '시군구')]


d2_aliases = """연기면:세종시 금남면:세종시 조치원읍:세종시 아산신:아산시 구로:구로구
종로2가:종로구"""
d2_aliases = dict(aliasset.split(':') for aliasset in d2_aliases.split())
ltr_tbl['d2'] = ltr_tbl['d2'].apply(lambda v: d2_aliases.get(v, v))
ltr_tbl['d2'].unique()

L = ltr_tbl.apply(lambda r: r['d1'] + ' ' + r['d2'], axis=1).value_counts()
L.head()
distr_latlon = pd.read_table('../../../../p/tiny/2014-12/burgerindex/latlon/lonlat.csv')
distr_latlon.head()


distr_latlon.index = distr_latlon.apply(lambda r: r['d1'] + ' ' + r['d2'], axis=1)
bgt = pd.DataFrame({'B': B, 'M': M, 'K': K, 'L': L}).fillna(0)
bgt = pd.merge(distr_latlon, bgt, how='outer', left_index=True, right_index=True)
bgt.head()

bgt[np.isnan(bgt['area'])]


bgidx_cols = ['B', 'K', 'L', 'M']
bgt.loc['경기도 여주시', bgidx_cols] += bgt.loc['경기도 여주군', bgidx_cols]
bgt.loc['울산광역시 울주군', bgidx_cols] += bgt.loc['울산광역시 을주군', bgidx_cols]
bgt.loc['충청남도 천안시', bgidx_cols] += bgt.loc['충청북도 천안시', bgidx_cols]
bgt.loc['충청북도 청주시', bgidx_cols] += bgt.loc['충청북도 청원군', bgidx_cols] # 2014년 7월 1일 통합.
bgt = bgt[~np.isnan(bgt['area'])].fillna(0)
bgt.head()


bgt[(bgt['L'] == 0) & (bgt['B'] + bgt['M'] + bgt['K'] > 0)]
bgt[bgt['L'] == 0]

bgt['BMK'] = bgt['B'] + bgt['M'] + bgt['K']
bgt['BgIdx'] = bgt['BMK'] / bgt['L']
bgt = bgt.sort('BgIdx', ascending=False)
bgt.head(10)


rcParams['font.family'] = 'NanumGothic'

plt.figure(figsize=(5, 5))
r = lambda: np.random.random(len(bgt))
plt.scatter(bgt['L'] + r(), bgt['BMK'] + r(), s=6, c='black', edgecolor='none', alpha=0.6)
plt.xlabel('롯데리아')
plt.ylabel('버거킹+맥도날드+KFC')
plt.xlim(0, 45)
plt.ylim(0, 45)
plt.gca().set_aspect(1)

# 추세선 그린다.
trendfun = np.poly1d(np.polyfit(bgt['L'], bgt['BMK'], 1))
trendx = np.linspace(0, 45, 2)
plt.plot(trendx, trendfun(trendx))

# 튀는 점 몇 개는 이름도 표시한다.
tolabel = bgt[(bgt['L'] > 17) | (bgt['BMK'] >= 14)]
for idx, row in tolabel.iterrows():
    label_name = idx.split()[1][:-1]
    plt.annotate(label_name, (row['L'], row['BMK']))


bgt.head()


def short_distr(name):
    wide, narrow = name.split()
    if narrow.endswith('구'):
        return wide[:2] + (narrow[:-1] if len(narrow) > 2 else narrow)
    elif narrow == '고성군': # 고성군은 강원도, 경상남도에 있다.
        return '고성({})'.format({'강원도': '강원', '경상남도': '경남'}[wide])
    else:
        return narrow[:-1]

bgt['shortname'] = list(map(short_distr, bgt.index))
bgt.head()


blockpositions = pd.read_csv('../../../../../p/tiny/2014-12/burgerindex/blockmap-positions.csv', names=range(15))
blockpositions.head()

flatrows = []
for y, colcities in blockpositions.iterrows():
    for x, city in colcities.iteritems():
        if isinstance(city, str):
            flatrows.append((x, y, city))

blockpositions_tbl = pd.DataFrame(flatrows, columns=('x', 'y', 'city')).set_index('city').sort_index()
bgtb = pd.merge(bgt, blockpositions_tbl, how='left', left_on='shortname', right_index=True)
bgtb.head()


bgtb[bgtb['x'].apply(np.isnan)]

from matplotlib import rcParams
from matplotlib import cm, colors, _cm
rcParams['font.family'] = 'NanumBarunGothic'

bgtb['BgIdx'] = bgtb['BgIdx'].fillna(0)



BORDER_LINES = [
    [(3, 2), (5, 2), (5, 3), (9, 3), (9, 1)], # 인천
    [(2, 5), (3, 5), (3, 4), (8, 4), (8, 7), (7, 7), (7, 9), (4, 9), (4, 7), (1, 7)], # 서울
    [(1, 6), (1, 9), (3, 9), (3, 10), (8, 10), (8, 9),
     (9, 9), (9, 8), (10, 8), (10, 5), (9, 5), (9, 3)], # 경기도
    [(9, 12), (9, 10), (8, 10)], # 강원도
    [(10, 5), (11, 5), (11, 4), (12, 4), (12, 5), (13, 5),
     (13, 4), (14, 4), (14, 2)], # 충청남도
    [(11, 5), (12, 5), (12, 6), (15, 6), (15, 7), (13, 7),
     (13, 8), (11, 8), (11, 9), (10, 9), (10, 8)], # 충청북도
    [(14, 4), (15, 4), (15, 6)], # 대전시
    [(14, 7), (14, 9), (13, 9), (13, 11), (13, 13)], # 경상북도
    [(14, 8), (16, 8), (16, 10), (15, 10),
     (15, 11), (14, 11), (14, 12), (13, 12)], # 대구시
    [(15, 11), (16, 11), (16, 13)], # 울산시
    [(17, 1), (17, 3), (18, 3), (18, 6), (15, 6)], # 전라북도
    [(19, 2), (19, 4), (21, 4), (21, 3), (22, 3), (22, 2), (19, 2)], # 광주시
    [(18, 5), (20, 5), (20, 6)], # 전라남도
    [(16, 9), (18, 9), (18, 8), (19, 8), (19, 9), (20, 9), (20, 10)], # 부산시
]
def draw_blockcolormap(tbl, datacol, vmin, vmax, whitelabelmin, cmapname, gamma, datalabel, dataticks):
    cmap = colors.LinearSegmentedColormap(cmapname + 'custom',
                      getattr(_cm, '_{}_data'.format(cmapname)), gamma=gamma)
    cmap.set_bad('white', 1.)

    mapdata = tbl.pivot(index='y', columns='x', values=datacol)
    masked_mapdata = np.ma.masked_where(np.isnan(mapdata), mapdata)

    plt.figure(figsize=(9, 16))
    plt.pcolor(masked_mapdata, vmin=vmin, vmax=vmax, cmap=cmap,
               edgecolor='#aaaaaa', linewidth=0.5)

    # 지역 이름 표시
    for idx, row in tbl.iterrows():
        annocolor = 'white' if row[datacol] > whitelabelmin else 'black'

        # 광역시는 구 이름이 겹치는 경우가 많아서 시단위 이름도 같이 표시한다. (중구, 서구)
        if row['d1'].endswith('시') and not row['d1'].startswith('세종'):
            dispname = '{}\n{}'.format(row['d1'][:2], row['d2'][:-1])
            if len(row['d2']) <= 2:
                dispname += row['d2'][-1]
        else:
            dispname = row['d2'][:-1]

        # 서대문구, 서귀포시 같이 이름이 3자 이상인 경우에 작은 글자로 표시한다.
        if len(dispname.splitlines()[-1]) >= 3:
            fontsize, linespacing = 12, 1.2
        else:
            fontsize, linespacing = 14, 1.03

        plt.annotate(dispname, (row['x']+0.5, row['y']+0.5), weight='bold',
                     fontsize=fontsize, ha='center', va='center', color=annocolor,
                     linespacing=linespacing)

    # 시도 경계 그린다.
    for path in BORDER_LINES:
        ys, xs = zip(*path)
        plt.plot(xs, ys, c='black', lw=2)

    plt.gca().invert_yaxis()
    plt.gca().set_aspect(1)

    plt.axis('off')
    
    cb = plt.colorbar(shrink=.1, aspect=10)
    cb.set_label(datalabel)
    cb.set_ticks(dataticks)

    plt.tight_layout()
#자, 이제 긴장되는 버거지수 지도 그리는 순간!

draw_blockcolormap(bgtb, 'BgIdx', 0, 3, 1.42, 'Blues', 0.75, '버거지수', np.arange(0, 3.1, 0.5))
plt.savefig('bmap-burgerindex.pdf')

bgtb['Lp10T'] = bgtb['L'] / bgtb['population'] * 10000
draw_blockcolormap(bgtb, 'Lp10T', 0, 1, 0.45, 'YlGn', 1, '1만명당 롯데리아 점포수', np.arange(0, 1.1, 0.2))
plt.savefig('bmap-lotteria.pdf')


bgtb['BMKp10T'] = bgtb['BMK'] / bgtb['population'] * 10000
draw_blockcolormap(bgtb, 'BMKp10T', 0, 1, 0.45, 'YlGn', 1, '1만명당 버거킹+맥도날드+KFC 점포수', np.arange(0, 1.1, 0.2))
plt.savefig('bmap-bmkshops.pdf')


bgtb['Bp10T'] = bgtb['B'] / bgtb['population'] * 10000
draw_blockcolormap(bgtb, 'Bp10T', 0, 0.5, 0.25, 'RdPu', 1, '1만명당 버거킹 점포수', np.arange(0, 0.6, 0.1))
plt.savefig('bmap-burgerking.pdf')


bgtb['Mp10T'] = bgtb['M'] / bgtb['population'] * 10000
draw_blockcolormap(bgtb, 'Mp10T', 0, 0.5, 0.25, 'RdPu', 1, '1만명당 맥도날드 점포수', np.arange(0, 0.6, 0.1))
plt.savefig('bmap-mcdonalds.pdf')

bgtb['Kp10T'] = bgtb['K'] / bgtb['population'] * 10000
draw_blockcolormap(bgtb, 'Kp10T', 0, 0.5, 0.25, 'RdPu', 1, '1만명당 KFC 점포수', np.arange(0, 0.6, 0.1))
plt.savefig('bmap-KFC.pdf')

bgtb['LBMKp10T'] = (bgtb['L'] + bgtb['BMK']) / bgtb['population'] * 10000
draw_blockcolormap(bgtb, 'LBMKp10T', 0, 2, 0.7, 'Oranges', 0.8, '1만명당 롯데리아/버거킹/맥도날드/KFC 점포수', np.arange(0, 2.1, 0.5))
plt.savefig('bmap-LBMK.pdf')

bgtb['logdensity'] = np.log10(bgtb['density'])
draw_blockcolormap(bgtb, 'logdensity', 0, 6, 4, 'Greens', 1, '인구밀도 (명/$km^2$)', np.arange(0, 6.1, 1))
plt.savefig('bmap-density.pdf')


draw_blockcolormap(bgtb, 'area', 0, 1500, 500, 'Greys', 0.6, '면적 ($km^2$)', np.arange(0, 510, 100))
plt.savefig('bmap-area.pdf')



plt.figure(figsize=(4, 3))
subcnt = bgt[['B', 'K', 'L', 'M']]
subcnt.columns = ['버거킹', 'KFC', '롯데리아', '맥도날드']
p = subcnt.sum(axis=0).plot(kind='bar')
plt.setp(p.get_xticklabels(), rotation=0)
plt.ylabel('매장 수')
plt.savefig('plot-shops-count.pdf')

fig = plt.figure(figsize=(9, 9))

def plot_nstores(b1, b2, label1, label2):
    plt.scatter(bgt[b1] + np.random.random(len(bgt)),
                bgt[b2] + np.random.random(len(bgt)),
                edgecolor='none', alpha=0.75, s=6, c='black')
    plt.xlim(-1, 15 if b1 != 'L' else 35)
    plt.ylim(-1, 15 if b2 != 'L' else 35)
    plt.xlabel(label1)
    plt.ylabel(label2)
    
    r = scipy.stats.pearsonr(bgt[b1], bgt[b2])
    plt.annotate('r={:.3f}'.format(r[0]), (9, 12.5), fontsize=14)

bgbrands = [
    ('B', '버거킹'), ('K', 'KFC'),
    ('L', '롯데리아'), ('M', '맥드날드'),
]

for a in range(len(bgbrands) - 1):
    for b in range(1, len(bgbrands)):
        if a >= b:
            continue
        ax = fig.add_subplot(len(bgbrands)-1, len(bgbrands)-1, a * 3 + b)
        acol, alabel = bgbrands[a]
        bcol, blabel = bgbrands[b]
        plot_nstores(bcol, acol, blabel, alabel)

plt.tight_layout()
plt.savefig('plot-shopcount-correlations.pdf')




cate = bgt.apply(lambda r: 'S' if (r['M'] <= 5) and (r['L'] <= 2) else (
                                'L' if r['M'] == 0 or r['L'] / r['M'] > 2.1 else 'M'),
                 axis=1)
colors = [{'S': 'gray', 'L': 'green', 'M': 'red'}[c] for c in cate]

plt.figure(figsize=(6, 6))
plt.scatter(bgt['M'] + np.random.random(len(bgt)),
            bgt['L'] + np.random.random(len(bgt)), s=8, c=colors, edgecolor='none')
			


fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(1, 1, 1)
bgt[cate == 'M'].plot(kind='scatter', x='population', y='density', ax=ax, c='red')
bgt[cate == 'L'].plot(kind='scatter', x='population', y='density', ax=ax, c='green')
plt.xscale('log')
plt.yscale('log')



scipy.stats.mannwhitneyu(bgt.loc[cate == 'M', 'population'], bgt.loc[cate == 'L', 'population'])



validbgt = bgt.dropna(subset=['BgIdx']).copy()
validbgt['logBgIdx'] = np.log2(validbgt['BgIdx'])
validbgt = validbgt.dropna(subset=['logBgIdx'])
validbgt['logDensity'] = np.log10(validbgt['density'])

fig = plt.figure(figsize=(4, 4))
ax = fig.add_subplot(1, 1, 1)
validbgt.plot(kind='scatter', y='logBgIdx', x='logDensity', ax=ax,
              edgecolor='none', s=8, c='black')
plt.xlabel('인구밀도 (log$_{10}$ 인/km$^2$)')
plt.ylabel('버거지수 (log$_2$)')

tau, taup = scipy.stats.kendalltau(validbgt['logBgIdx'], validbgt['logDensity'])
print("Kendall's tau: {} (p={})".format(tau, taup))
plt.annotate('$\\tau$ = {:.3f}'.format(tau), (2, 2), fontsize=14)





lotteria_per_population = bgt['L'].sum() / bgt['population'].sum()
lotteria_to_random = bgt['L'] / (lotteria_per_population * bgt['population'])

BK_per_population = (bgt['B'] + bgt['K']).sum() / bgt['population'].sum()
BK_to_random = (bgt['B'] + bgt['K']) / (BK_per_population * bgt['population'])

mcdonalds_per_population = bgt['M'].sum() / bgt['population'].sum()
mcdonalds_to_random = bgt['M'] / (mcdonalds_per_population * bgt['population'])






import rpy2.robjects as ro

def loess_fit(x, y, px=None, model=None, alpha=0.5):
    if model is None:
        model = ro.r('y ~ x')

    if px is None:
        px = np.linspace(min(x), max(x), 22)[1:-1]

    fitframe = ro.DataFrame({'x': ro.FloatVector(x), 'y': ro.FloatVector(y)})
    loessmodel = ro.r.loess(model, fitframe, span=alpha)

    predframe = ro.DataFrame({'x': ro.FloatVector(px)})
    predy = ro.r.predict(loessmodel, predframe)
    preddata = [(x, predy[i]) for i, x in enumerate(px)]

    return np.array(preddata).transpose()

lotteria_trend = loess_fit(np.log10(bgt['density']), lotteria_to_random)
BK_trend = loess_fit(np.log10(bgt['density']), BK_to_random)
mcdonalds_trend = loess_fit(np.log10(bgt['density']), mcdonalds_to_random)
fig = plt.figure(figsize=(10, 4))
ax = fig.add_subplot(1, 3, 1)
plt.scatter(np.log10(bgt['density']), lotteria_to_random, s=6, c='black', edgecolor='none')
plt.axhline(1, lw=2, c='black', alpha=0.2)
plt.plot(lotteria_trend[0], lotteria_trend[1], c='red', alpha=0.7, lw=1.5, zorder=3)
plt.ylim(-0.1, 7.0)
plt.xlabel('인구밀도 (log10 인/$km^2$)')
plt.ylabel('기대 매장 수 대비 실제 매장 수 비율')
plt.title('롯데리아')

ax = fig.add_subplot(1, 3, 2)
plt.scatter(np.log10(bgt['density']), BK_to_random, s=6, c='black', edgecolor='none')
plt.plot(BK_trend[0], BK_trend[1], c='red', alpha=0.7, lw=1.5, zorder=3)
plt.axhline(1, lw=2, c='black', alpha=0.2)
plt.ylim(-0.1, 7.0)
plt.xlabel('인구밀도 (log10 인/$km^2$)')
plt.ylabel('기대 매장 수 대비 실제 매장 수 비율')
plt.title('버거킹+KFC')

ax = fig.add_subplot(1, 3, 3)
plt.scatter(np.log10(bgt['density']), mcdonalds_to_random, s=6, c='black', edgecolor='none')
plt.plot(mcdonalds_trend[0], mcdonalds_trend[1], c='red', alpha=0.7, lw=1.5, zorder=3)
plt.axhline(1, lw=2, c='black', alpha=0.2)
plt.ylim(-0.1, 7.0)
plt.xlabel('인구밀도 (log10 인/$km^2$)')
plt.ylabel('기대 매장 수 대비 실제 매장 수 비율')
plt.title('맥도날드')

plt.tight_layout()






def sim_positions(nstores):
    simulated_nstores = pd.Series([0] * len(bgt), index=bgt.index)

    for i in range(int(nstores)):
        maxloc = (bgt['population'] / (simulated_nstores + 1)).argmax()
        simulated_nstores.loc[maxloc] += 1

    return simulated_nstores

sim_BMK = sim_positions(bgt['B'].sum()) + sim_positions(bgt['M'].sum()) + sim_positions(bgt['K'].sum())
sim_L = sim_positions(bgt['L'].sum())

plt.scatter(bgt['BgIdx'] + np.random.normal(0, 0.05, len(bgt)),
            sim_BMK / sim_L + np.random.normal(0, 0.05, len(bgt)),
            s=5, c='black')
valid = (np.isfinite(bgt['BgIdx']) & (bgt['L'] > 0) & (sim_L > 0) & (sim_BMK > 0))
pr, pp = scipy.stats.pearsonr(bgt['BgIdx'][valid], sim_BMK[valid] / sim_L[valid])
plt.ylim(-0.2, 1)
plt.xlim(-0.2, 5)
plt.gca().set_aspect(1)
plt.xlabel('실제 버거지수')
plt.ylabel('시뮬레이션 결과')
plt.annotate('r={:.3f}'.format(pr), (3, 0.2))






import requests

url = '크롤링 대상 페이지'
req = requests.get(url)
print(req.encodng)   #'ISO-8859-1'



req.encoding = 'utf-8'    #인코딩 방식을 utf-8 로 바꿉니다. 

html = req.text







# 한글폴더 Pandas 입력 에러시
import os

fname = './폴더/파일명.csv'
fname_change = ''
for c in fname:
    if ord(c) <= 255:
        fname_change += c

pd = pd.read_csv(fname_change)





https://m.blog.naver.com/PostView.nhn?blogId=kiddwannabe&logNo=221156319157&navType=tl





# requests를 활용하여 온라인상 이미지/자료 다운받기
  
import requests

url='다운받을자료주소.jpg'
savename='파일명.png'
response=requests.get(url)

with open(savename, "wb") as outfile:
    outfile.write(response.content)





# a 라는 리스트에 긴 숫자를 넣어줌
a=[]
for i in range(10000000):
    a.append(i)
     
%time for k in range(1000): 50*k+1000 in a    
     # 리스트에서 존재여부 확인할 경우 Wall time: 450 ms

b=set(a)
%time for k in range(1000): 50*k+1000 in b 
    # Set으로 변경후 존재여부 확인할 경우  Wall time: 0 ns




예시1)
  def wage_range(x):
    if x>=6000:
        return '6천원 이상'
    elif x>=5000:
        return '5천원~6천원'
    else:
        return '5천원 미만'

data['시급분류']=data.hourly_wage.map(lambda x: wage_range(x))


예시2)

def make_brands(x):
    return str(x)[:5]

products['브랜드코드']=products.자재코드.map(lambda x: make_brands(x))

colum 이름 변경하기

data=data.rename(columns={"기존이름1":"변경이름1", "기존이름2":"변경이름2"})
 ex) pegi_done=pegi_done.rename(columns={"수량":"폐기완료수량"})





import itertools

def find_sublists(seq, sublist):
    length = len(sublist)
    for index, value in enumerate(seq):
        if value == sublist[0] and seq[index:index+length] == sublist:
            yield index, index+length      
            # input 이 업데이트될때마다 새롭게 값을 찾아서 리턴해줌

def replace_sublist(seq, target, replacement, maxreplace=None):
    sublists = find_sublists(seq, target)
    if maxreplace:
        sublists = itertools.islice(sublists, maxreplace)
    for start, end in sublists:
        seq[start:end] = replacement

활용예)

import itertools

def find_sublists(seq, sublist):
    length = len(sublist)
    for index, value in enumerate(seq):
        if value == sublist[0] and seq[index:index+length] == sublist:
            yield index, index+length      
            # input 이 업데이트될때마다 새롭게 값을 찾아서 리턴해줌

def replace_sublist(seq, target, maxreplace=None):
    sublists = find_sublists(seq, target)
    if maxreplace:
        sublists = itertools.islice(sublists, maxreplace)
    for start, end in sublists:
        seq[start:end] = []

def main():
    replace_sublist(changed_exp,["N","?"])
    replace_sublist(changed_exp,["?","N"])
    replace_sublist(changed_exp,["?"])



https://www.it-swarm.dev/ko/python/%ED%8C%90%EB%8B%A4%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%ED%94%8C%EB%A1%AF-%EC%83%81%EA%B4%80-%ED%96%89%EB%A0%AC/1052561209/



https://sikaleo.tistory.com/40



df[df['A'].isin([3, 6])]

df[~df['A'].isin([3, 6])]

	
	
	






xgboost 소개 : 오늘코드 

전체 보기 : http://bit.ly/inflearn_nlp_tutorial

소스코드 : https://github.com/corazzon/KaggleStr...

# XGBoost 소개, 캐글 우승자 인터뷰 소개, 부스팅 알고리즘을 사용해서 점수를 올려보기

* 2015년 캐글 블로그에 xgboost를 사용하여 17건의 우승 솔루션이 공유됨 : http://blog.kaggle.com/2015/12/03/dat...

* 2016년 논문이 등록 됨 : http://dmlc.cs.washington.edu/data/pd...
*  공식문서 : https://xgboost.readthedocs.io/en/lat...

* 분산형 그래디언트 부스팅 알고리즘
* 부스팅 알고리즘은?
    * 부스팅 알고리즘은 약한 예측모형들을 결합하여 강한 예측모형을 만드는 알고리즘
    * 배깅과 유사하게 초기 샘플데이터로 다수의 분류기를 만들지만 배깅과 다르게 순차적이다.
    * 랜덤포레스트의 배깅과는 다르게 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듦
    * 결정트리(Decision Tree) 알고리즘의 연장선에 있음
    * 여러 개의 결정트리를 묶어 강력한 모델을 만드는 앙상블 방법
    * 분류와 회귀에 사용할 수 있음
    * 무작위성이 없으며 강력한 사전 가지치기를 사용
    * 참고 이미지 : http://www.birc.co.kr/2017/02/06/%EC%...
    * 배깅과 부스팅의 차이점은 udacity에서 설명한 영상이 가장 도움이 되었음
        * 배깅 : https://www.youtube.com/watch?v=2Mg8Q...
        * 부스팅 : https://www.youtube.com/watch?v=GM3CD...
* 타이타닉 경진대회에 사용 예제가 있음


pandas_fcmp.ipynb참고



### multiprocess

https://docs.python.org/ko/3/library/multiprocessing.html

